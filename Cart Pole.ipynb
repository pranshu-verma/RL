{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02387893,  0.00539569, -0.03564763, -0.00441803])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(4,)\n",
      "Action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.state_dim = env.observation_space.shape\n",
    "        self.action_size = env.action_space.n\n",
    "        self.weights = np.random.randn(self.state_dim[0], self.action_size)\n",
    "        self.optimal_weights = np.copy(self.weights)\n",
    "        self.best_reward = -1e4\n",
    "        self.noise = 0.1\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        action = np.argmax(np.dot(state, self.weights))\n",
    "        return action\n",
    "    \n",
    "    def update_model(self, reward):\n",
    "        if reward >= self.best_reward:\n",
    "            self.best_reward = reward\n",
    "            self.best_weights = np.copy(self.weights)\n",
    "            self.noise /= 2\n",
    "        else:\n",
    "            self.noise *= 1.5\n",
    "        \n",
    "        self.weights = self.best_weights + self.noise * np.random.rand(self.state_dim[0], self.action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 finished with a reward: 17.0\n",
      "Episode: 2 finished with a reward: 40.0\n",
      "Episode: 3 finished with a reward: 15.0\n",
      "Episode: 4 finished with a reward: 25.0\n",
      "Episode: 5 finished with a reward: 30.0\n",
      "Episode: 6 finished with a reward: 22.0\n",
      "Episode: 7 finished with a reward: 49.0\n",
      "Episode: 8 finished with a reward: 14.0\n",
      "Episode: 9 finished with a reward: 40.0\n",
      "Episode: 10 finished with a reward: 27.0\n",
      "Episode: 11 finished with a reward: 24.0\n",
      "Episode: 12 finished with a reward: 14.0\n",
      "Episode: 13 finished with a reward: 25.0\n",
      "Episode: 14 finished with a reward: 39.0\n",
      "Episode: 15 finished with a reward: 27.0\n",
      "Episode: 16 finished with a reward: 17.0\n",
      "Episode: 17 finished with a reward: 9.0\n",
      "Episode: 18 finished with a reward: 69.0\n",
      "Episode: 19 finished with a reward: 39.0\n",
      "Episode: 20 finished with a reward: 44.0\n",
      "Episode: 21 finished with a reward: 77.0\n",
      "Episode: 22 finished with a reward: 26.0\n",
      "Episode: 23 finished with a reward: 43.0\n",
      "Episode: 24 finished with a reward: 9.0\n",
      "Episode: 25 finished with a reward: 48.0\n",
      "Episode: 26 finished with a reward: 39.0\n",
      "Episode: 27 finished with a reward: 86.0\n",
      "Episode: 28 finished with a reward: 128.0\n",
      "Episode: 29 finished with a reward: 148.0\n",
      "Episode: 30 finished with a reward: 73.0\n",
      "Episode: 31 finished with a reward: 75.0\n",
      "Episode: 32 finished with a reward: 29.0\n",
      "Episode: 33 finished with a reward: 109.0\n",
      "Episode: 34 finished with a reward: 126.0\n",
      "Episode: 35 finished with a reward: 10.0\n",
      "Episode: 36 finished with a reward: 10.0\n",
      "Episode: 37 finished with a reward: 10.0\n",
      "Episode: 38 finished with a reward: 34.0\n",
      "Episode: 39 finished with a reward: 9.0\n",
      "Episode: 40 finished with a reward: 40.0\n",
      "Episode: 41 finished with a reward: 20.0\n",
      "Episode: 42 finished with a reward: 40.0\n",
      "Episode: 43 finished with a reward: 10.0\n",
      "Episode: 44 finished with a reward: 8.0\n",
      "Episode: 45 finished with a reward: 8.0\n",
      "Episode: 46 finished with a reward: 12.0\n",
      "Episode: 47 finished with a reward: 48.0\n",
      "Episode: 48 finished with a reward: 9.0\n",
      "Episode: 49 finished with a reward: 44.0\n",
      "Episode: 50 finished with a reward: 10.0\n",
      "Episode: 51 finished with a reward: 158.0\n",
      "Episode: 52 finished with a reward: 159.0\n",
      "Episode: 53 finished with a reward: 68.0\n",
      "Episode: 54 finished with a reward: 83.0\n",
      "Episode: 55 finished with a reward: 165.0\n",
      "Episode: 56 finished with a reward: 192.0\n",
      "Episode: 57 finished with a reward: 200.0\n",
      "Episode: 58 finished with a reward: 123.0\n",
      "Episode: 59 finished with a reward: 101.0\n",
      "Episode: 60 finished with a reward: 144.0\n",
      "Episode: 61 finished with a reward: 38.0\n",
      "Episode: 62 finished with a reward: 200.0\n",
      "Episode: 63 finished with a reward: 191.0\n",
      "Episode: 64 finished with a reward: 44.0\n",
      "Episode: 65 finished with a reward: 200.0\n",
      "Episode: 66 finished with a reward: 200.0\n",
      "Episode: 67 finished with a reward: 200.0\n",
      "Episode: 68 finished with a reward: 200.0\n",
      "Episode: 69 finished with a reward: 200.0\n",
      "Episode: 70 finished with a reward: 200.0\n",
      "Episode: 71 finished with a reward: 200.0\n",
      "Episode: 72 finished with a reward: 200.0\n",
      "Episode: 73 finished with a reward: 200.0\n",
      "Episode: 74 finished with a reward: 200.0\n",
      "Episode: 75 finished with a reward: 200.0\n",
      "Episode: 76 finished with a reward: 200.0\n",
      "Episode: 77 finished with a reward: 200.0\n",
      "Episode: 78 finished with a reward: 200.0\n",
      "Episode: 79 finished with a reward: 200.0\n",
      "Episode: 80 finished with a reward: 200.0\n",
      "Episode: 81 finished with a reward: 200.0\n",
      "Episode: 82 finished with a reward: 200.0\n",
      "Episode: 83 finished with a reward: 200.0\n",
      "Episode: 84 finished with a reward: 200.0\n",
      "Episode: 85 finished with a reward: 200.0\n",
      "Episode: 86 finished with a reward: 200.0\n",
      "Episode: 87 finished with a reward: 200.0\n",
      "Episode: 88 finished with a reward: 200.0\n",
      "Episode: 89 finished with a reward: 200.0\n",
      "Episode: 90 finished with a reward: 200.0\n",
      "Episode: 91 finished with a reward: 200.0\n",
      "Episode: 92 finished with a reward: 200.0\n",
      "Episode: 93 finished with a reward: 200.0\n",
      "Episode: 94 finished with a reward: 200.0\n",
      "Episode: 95 finished with a reward: 200.0\n",
      "Episode: 96 finished with a reward: 200.0\n",
      "Episode: 97 finished with a reward: 200.0\n",
      "Episode: 98 finished with a reward: 200.0\n",
      "Episode: 99 finished with a reward: 200.0\n",
      "Episode: 100 finished with a reward: 200.0\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(env)\n",
    "episodes = 100\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#         env.render()\n",
    "        total_reward += reward\n",
    "    \n",
    "    agent.update_model(total_reward)\n",
    "    print(f\"Episode: {episode+1} finished with a reward: {total_reward}\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
