{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "env = env.unwrapped\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "episodes = 0\n",
    "\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    episodes += 1\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"Finished after {} episodes\".format(episodes))\n",
    "print('observation:', observation)\n",
    "print('reward:', reward)\n",
    "print('done:', done)\n",
    "print('info:', info)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box2discrete(env, obs, num_states=20):\n",
    "    pos_discrete = ((env.observation_space.high - env.observation_space.low) / num_states)[0]\n",
    "    vel_discrete = ((env.observation_space.high - env.observation_space.low) / num_states)[1]\n",
    "    new_pos = int((obs[0] - env.observation_space.low[0]) / pos_discrete)\n",
    "    new_vel = int((obs[1] - env.observation_space.low[1]) / vel_discrete)\n",
    "    return new_pos, new_vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = 20\n",
    "q_table = np.zeros([num_states, num_states, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(box2discrete(env, env.reset()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 1000\n",
    "lr_start = 1\n",
    "lr_end = 0.005\n",
    "# learning_rate = 0.01\n",
    "discount = 0.95\n",
    "epsilon = 0.05\n",
    "\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    learning_rate = max(lr_end, lr_start * (discount ** (episode // 100)))\n",
    "    \n",
    "    while True:\n",
    "        pos, vel = box2discrete(env, state)\n",
    "        if np.random.uniform(0, 1) > epsilon:\n",
    "            action = np.argmax(q_table[pos][vel])\n",
    "        else:\n",
    "            action = np.random.choice(env.action_space.n)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        new_pos, new_vel = box2discrete(env, new_state)\n",
    "        \n",
    "        q_table[pos][vel][action] = (1 - learning_rate) * q_table[pos][vel][action] + learning_rate * (reward + discount * np.max(q_table[new_pos][new_vel]))\n",
    "       \n",
    "        state = new_state\n",
    "        \n",
    "        if reward == 1.0:\n",
    "            print(\"WON\")\n",
    "        if done:\n",
    "            break   \n",
    "\n",
    "# print(q_table[pos][vel][action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10\n",
    "max_steps = 200\n",
    "won = 0\n",
    "\n",
    "for episode in range(episodes):\n",
    "    clear_output(wait=True)\n",
    "    state = env.reset()\n",
    "    pos, vel = box2discrete(env, state)\n",
    "    done = False\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        action = np.argmax(q_table[pos][vel])\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        print(\"*\" * 10)\n",
    "        print(f\"Episode: {episode+1}\")\n",
    "        print(f\"Step {step+1}\")\n",
    "        print(f\"Result {won}/{episode}\")\n",
    "        print(f\"Reward: {reward}\")\n",
    "        env.render()\n",
    "#         time.sleep(0.2)\n",
    "        state = new_state\n",
    "        \n",
    "        if reward == 1.0:\n",
    "            won += 1\n",
    "            print(\"You reached the goal\")\n",
    "            break\n",
    "        \n",
    "        if done and not reward:\n",
    "            print(\"You fell in the hole\")\n",
    "            break\n",
    "        \n",
    "    else:\n",
    "        print(f\"Maximum step ({max_steps}) limit reached.\")\n",
    "        \n",
    "    time.sleep(2)\n",
    "#     clear_output(wait=True)\n",
    "\n",
    "print(f\"Percentage of winning is {(won / episodes) * 100}%\")\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
